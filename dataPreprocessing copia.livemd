<!-- livebook:{"file_entries":[{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/qianqian/TPCDataAnalysis/RealData/answers-aaf7180-2025-04-22 14_17_28.csv"},"name":"answers-aaf7180-2025-04-22_14_17_28.csv","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/qianqian/TPCDataAnalysis/RealData/events-aaf7180-2025-04-22 14_26_18.csv"},"name":"events-aaf7180-2025-04-22_14_26_18.csv","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/qianqian/tpcassets/priv/repo/new03.json"},"name":"new03.json","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/qianqian/TPCDataAnalysis/RealData/participation-aaf7180-2025-04-22 14_17_51.csv"},"name":"participation-aaf7180-2025-04-22_14_17_51.csv","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/qianqian/profile-clustering-4.csv"},"name":"profile-clustering-4.csv","type":"file"}]} -->

# DataAnalysis

```elixir
Mix.install([
  {:req, "~> 0.4.8"},
  {:explorer, "~> 0.8.0"},
  {:kino_explorer, "~> 0.1.18"},
  {:vega_lite, "~> 0.1.8"},
  {:kino_vega_lite, "~> 0.1.3"},
  {:tzdata, "~> 1.1"},
  {:nx, "~> 0.7.0"},
  {:scholar, "~> 0.3.0"}
])
```

## Section

```elixir
require Explorer.DataFrame, as: DF
require Explorer.Series, as: Series
require VegaLite, as: Vl
```

For simplicity and flexibility purposes, let's define the file paths to all the csv files that is needed.

```elixir
defmodule Constants do
  @answers_file_path "/Users/qianqian/TPCDataAnalysis/RealData/answers-aaf7180-2025-04-22 14_17_28.csv"
  @events_file_path "/Users/qianqian/TPCDataAnalysis/RealData/events-aaf7180-2025-04-22 14_26_18.csv"
  @participation_file_path "/Users/qianqian/TPCDataAnalysis/RealData/participation-aaf7180-2025-04-22 14_17_51.csv"
  @clustering_label_file_path "/Users/qianqian/profile-sub-clustering-2.csv"
  
  def get_answers_file_path, do: @answers_file_path
  def get_events_file_path, do: @events_file_path
  def get_participation_file_path, do: @participation_file_path
  def get_clustering_label_file_path, do: @clustering_label_file_path
end
```

## Session 1: First Inspection of csv Data

Initial inspection into **answers.csv**.

```elixir
answers_df = DF.from_csv!(Constants.get_answers_file_path())
IO.inspect(answers_df)
```

Throughout the online course, students were asked to answer some questionnaires. Initial questionnaire to understand the basic information of the participants (such as the country of origin, gender, knowledge level of technology, etc). Other questionnaire are also included halfway through the course, to understand how they have being doing it. Lastly, a final questionnaire is included to ask regarding their final experience. This csv file contains all the answers responded by the participants.

Different questions are divided into different categories, such as:

1. **Profile** - Contains some initial questions that are asked at the beginning of the course to gather some basic information of the students
2. **Access** - The level of access to the technology. These are asked at the beginning of the course
3. **selfpre** - The level of knowledge on the technology at the beginning of the course.
4. **Feedback** - Contains information for feedback after finishing up a unit. This can be after watching a video, etc.
5. **Selfpost** - The level of knowledge after finishing the course
6. **Satisfaction** - The level of satisfaccion after finishing the course
7. **Usability** - Usability level of the entire course, hence it is asked after finishing the course

## Gathering student profiles

```elixir
profile_questions_list = 
  answers_df
  |> DF.select(["Categoría de la pregunta"])
  |> DF.distinct(["Categoría de la pregunta"])
  |> DF.to_rows()
```

Just like we mentioned, we have different questionnaires throughout the course. What we are interested are mainly the student profile, where it includes all the academic backgrounds, etc. Especially when we want to perform a clustering technique. Therefore, I will retrieve those information independently, forming a unique dataset. By analyzing this dataset individually can also provide us an insight on the type of students that we have.

```elixir
#This is a module used to generate a row where it represents the participant's answers
defmodule ParticipantRowBuilder do
  alias Explorer.DataFrame, as: DF
  alias Explorer.Series, as: Series

  def build_participant_row_data(participant, answers_df, new_df_col_names) do
    participant_code = participant["ID de participante"]

    # Filter the answers for this participant
    participant_answers =
      DF.filter_with(answers_df, fn answer ->
        Series.equal(answer["ID de participante"], participant_code)
      end)
      |> DF.to_rows()

    Enum.reduce(new_df_col_names, %{}, fn col, acc ->
      cond do
        col == "Participant ID" ->
          Map.put(acc, col, participant_code)

        true ->
          answer_row =
            Enum.find(participant_answers, fn q ->
              q["Texto de la pregunta"] == col
            end)

          answer =
            if answer_row do
              answer_row["Selección en la respuesta"]
            else
              -1
            end
          Map.put(acc, col, answer)
      end
    end)
  end
end

```

```elixir
profile_questions_list = 
  answers_df
  |> DF.filter(col("Categoría de la pregunta") == "profile")
  |> DF.select(["Texto de la pregunta"])
  |> DF.distinct(["Texto de la pregunta"])
  |> DF.to_rows()

profile_questions_list = Enum.map(profile_questions_list, & &1["Texto de la pregunta"])


#IO.inspect(profile_questions_list)# We have 13 questions that belong to the profile questions 
column_values = ["Participant ID" | profile_questions_list]
#Generate a profile dataset, where each row represents the profile of the participants


# Get all unique participants - 878 in total
participants_list =
  answers_df
  |> DF.select(["ID de participante"])
  |> DF.distinct(["ID de participante"])
  |> DF.to_rows()


participants_profile_rows =
  Enum.map(participants_list, fn row ->
    ParticipantRowBuilder.build_participant_row_data(row, answers_df, column_values)
  end)

student_profile_df = DF.new(participants_profile_rows)
#DF.filter(student_profile_df, col("Participant ID") == "bb594769-ccfb-4e71-af22-609449418fcf")
```

## Gathering answers for initial questionnaire

```elixir
init_questions_list = 
  answers_df
  |> DF.filter(col("Categoría de la pregunta") == "profile" 
    or col("Categoría de la pregunta") == "access" or col("Categoría de la pregunta") == "selfpre" )
  |> DF.select(["Texto de la pregunta"])
  |> DF.distinct(["Texto de la pregunta"])
  |> DF.to_rows()

init_questions_list = Enum.map(init_questions_list, & &1["Texto de la pregunta"])

IO.inspect(length(init_questions_list)) # We have 29 questions that were asked in the initial questionnaire questions 
column_values = ["Participant ID" | init_questions_list]
#Generate a profile dataset, where each row represents the profile of the participants

participants_profile_rows =
  Enum.map(participants_list, fn row ->
    ParticipantRowBuilder.build_participant_row_data(row, answers_df, column_values)
  end)

init_answers_df = DF.new(participants_profile_rows)
```

## Gather answers for all questions in the csv files

```elixir
# Get all unique questions
questions_df = DF.distinct(answers_df, ["Texto de la pregunta"])
questions_list = DF.to_series(questions_df) |> then(&Series.to_list(&1["Texto de la pregunta"]))
IO.inspect(length(questions_list)) #55 questions in total
column_values = ["Participant ID" | questions_list]

# Build a list of maps for each participant
participant_rows =
  Enum.map(participants_list, fn row ->
    ParticipantRowBuilder.build_participant_row_data(row, answers_df, column_values)
  end)

# Build the final DataFrame all at once
questions_df = DF.new(participant_rows)
```

## Checking for questions that are not answered by the students

Questions not answered by the student could potentially means either the student didn't wants to answer, or the student having difficulties of answering the questions. By having this in mind, we will not discard them. In fact we will make of them and consider them as a variable resource when predicting which questions will more likely to be not responded by the student.

"We realised that not all the questions are answered by the students. Hence we discard them." -> This might be removed

```elixir
long_form =
  DF.to_rows(questions_df)
  |> Enum.flat_map(fn row ->
    student_id = row["Participant ID"]
    IO.inspect(student_id)

    row
    |> Enum.reject(fn {key, _} -> key == "Participant ID" end)  # Remove "Participant ID"
    |> Enum.map(fn {question_id, answer} ->
      %{
        "StudentID" => student_id,
        "QuestionID" => question_id,
        "Answer" => answer,
        "Answered" => if answer == -1 do 0 else 1 end
      }
      end)
  end)
  |> DF.new()

```

We observe that the "Texto de la respuesta" + "Seleccion en la respuesta" represents the same piece of informaiton. Moreover, we are using Ml models later on where "Texto de la respuesta" seems to be more useful. We remove "Seleccion en la respuesta"

```elixir
answers_df = DF.discard(answers_df, ["Texto de la respuesta"])
```

## Check for student's age distribution

```elixir
student_age_df = DF.filter_with(answers_df, fn answers_df -> Explorer.Series.equal(answers_df["ID de pregunta"], "a30f78cc-f230-4851-a98d-5aaf2bf8bc19") end)

grouped_age_df =
  student_age_df
    |> DF.group_by("Selección en la respuesta")
    |> DF.summarise_with(&[count: Series.count(&1["Selección en la respuesta"])])

#Draw bar chart 
bar_chart = 
Vl.new(width: 600, height: 400)
|> Vl.data_from_values(grouped_age_df)
|> Vl.mark(:bar, clip: true)
|> Vl.encode_field(:x, "Selección en la respuesta", title: "Age Range", axis: [label_angle: 0])
|> Vl.encode_field(:y, "count",
  type: :quantitative,
  title: "Count",
  scale: [domain: [0, 280]]
)

Kino.VegaLite.new(bar_chart)
```

## Inspection into events.csv

```elixir
events_df = DF.from_csv!(Constants.get_events_file_path(), dtypes: %{"Participante" => :string})

```

From the this dataset, we can see that it contains the time of when the student enteres and exits the platform (recorded in the column of "Ocurrío en"). With this piece of information we can generate some interesting data that can be useful for our model:

1. Each time a student enters the platform, how long do they stay? Longer time on the platform may refer to better student engagement
2. How frequently does a student enters to the platform? As in, the number of "entered" event. A higher frequency may reflect to higher engagement level

Let's generate the data from the 1st point. To start with let's observe the data type of each column.

```elixir
DF.dtypes(events_df)
```

We can see that the "Ocurrío en" column contains a string datatype.

In Elixir, there is an interesting module DateTime that represents a special data structure for date and time. More interestingly, there is a built-in function that directly computes the time differences in seconds. To generate this data structure, we need different information such as year, month, date, time, timezone, etc. So what we need to do first is to gather all this information from the string. Hence, a special module is created, called "TimeParser"

```elixir
defmodule TimeParser do
  @monthRegex ~r/^\d{4}-(\d{2})-\d{2}/
  @dateRegex ~r/^\d{4}-\d{2}-(\d{2})/
  @yearRegex ~r/^(\d{4})-\d{2}-\d{2}/
  @hourRegex ~r/(\d{2}):\d{2}:\d{2}/
  @minuteRegex ~r/\d{2}:(\d{2}):\d{2}/
  @secondRegex ~r/\d{2}:\d{2}:(\d{2}).\d/
  @microsecondRegex ~r/\d{2}:\d{2}:\d{2}.(\d{6})/
  @timezoneRegex ~r/\s[A-Za-z]+\s([\w\/]+)$/
  
  def get_month(time_text) do
    month_result = Regex.run(@monthRegex, time_text)
    month_result = Enum.at(month_result, 1)
    String.to_integer(month_result)
  end

  def get_date(time_text) do
    date_result = Regex.run(@dateRegex, time_text)
    date_result = Enum.at(date_result, 1)
    String.to_integer(date_result)
  end

  def get_year(time_text) do
    year_result = Regex.run(@yearRegex, time_text)
    year_result = Enum.at(year_result, 1)
    String.to_integer(year_result)
  end

  def get_hour(time_text) do
    hour_result = Regex.run(@hourRegex, time_text)
    hour_result = Enum.at(hour_result, 1)
    String.to_integer(hour_result)
  end

  def get_minute(time_text) do
    minute_result = Regex.run(@minuteRegex, time_text)
    minute_result = Enum.at(minute_result, 1)
    String.to_integer(minute_result)
  end

  def get_second(time_text) do
    second_result = Regex.run(@secondRegex, time_text)
    second_result = Enum.at(second_result, 1)
    String.to_integer(second_result)
  end

  def get_timezone(time_text) do
    timezone_result = Regex.run(@timezoneRegex, time_text)
    Enum.at(timezone_result, 1)
  end

  def get_microsecond(time_text) do
    microsecond_result = Regex.run(@microsecondRegex, time_text)
    microsecond_result = Enum.at(microsecond_result, 1)
    String.to_integer(microsecond_result)
  end
end

#IO.puts(TimeParser.get_timezone("2025-03-21 12:16:33.518269+01:00 CET Europe/Madrid"))
```

```elixir
#define a function to determine the time differences in seconds
generate_dateTime_diff_from_str = fn str1, str2 ->
  entered_date = Date.new!(TimeParser.get_year(str1), TimeParser.get_month(str1), TimeParser.get_date(str1))
  entered_time = Time.new!(TimeParser.get_hour(str1), TimeParser.get_minute(str1), TimeParser.get_second(str1), TimeParser.get_microsecond(str1))  
  entered_dateTime = DateTime.new!(entered_date, entered_time, TimeParser.get_timezone(str1), Tzdata.TimeZoneDatabase)
  #get the exited date
  exited_date = Date.new!(TimeParser.get_year(str2), TimeParser.get_month(str2), TimeParser.get_date(str2))
  exited_time = Time.new!(TimeParser.get_hour(str2), TimeParser.get_minute(str2), TimeParser.get_second(str2), TimeParser.get_microsecond(str2))
  exited_dateTime = DateTime.new!(exited_date, exited_time, TimeParser.get_timezone(str2), Tzdata.TimeZoneDatabase)

  DateTime.diff(entered_dateTime, exited_dateTime)
end

video_df = DF.filter_with(events_df, fn events_df -> Explorer.Series.equal(events_df["Tipo de actividad"], "video") end)

enter_exit_df =
  DF.filter_with(video_df, fn df ->
    col = df["Tipo de evento"]
    cond_a = Explorer.Series.equal(col, "entered")
    cond_b = Explorer.Series.equal(col, "exited")
    Explorer.Series.or(cond_a, cond_b)
  end)

video_df = 
  enter_exit_df
  |>DF.group_by( ["ID Actividad", "Participante"])

distinct_student_activities = 
  enter_exit_df
  |> DF.distinct(["ID Actividad","Participante"])
  |> DF.to_rows()
  |> Enum.map(fn %{"ID Actividad" => current_id, "Participante" => current_participant} ->
    df = 
      enter_exit_df
      |> DF.filter_with(fn enter_exit_df -> Explorer.Series.equal(enter_exit_df["ID Actividad"], current_id) end)
      |> DF.filter_with(fn enter_exit_df -> Explorer.Series.equal(enter_exit_df["Participante"], current_participant) end)

    entered_time = 
      df
      |> DF.filter_with(fn df -> Explorer.Series.equal(df["Tipo de evento"], "entered") end)
      |> DF.sort_with(&[&1["Ocurrió en"]])
      |> DF.head(1)
      |> DF.to_rows()
      |> List.first()
      |> Map.get("Ocurrió en")

    exited_time =
      df
      |> DF.filter_with(fn df -> Explorer.Series.equal(df["Tipo de evento"], "exited") end)
      |> DF.sort_with(&[desc: &1["Ocurrió en"]])
      |> DF.head(1)
      |> DF.to_rows()
      |> List.first()
      |> case do
        nil -> 
          current_time = DateTime.now!("Etc/UTC") 
          #convert current time to CET
          current_time = DateTime.shift_zone!(current_time, "Europe/Madrid", Tzdata.TimeZoneDatabase)
          DateTime.to_string(current_time)
        row -> Map.get(row, "Ocurrió en")  
      end

    time_diff_sec = generate_dateTime_diff_from_str.(exited_time, entered_time)
    
    %{
      id: current_id,
      participant: current_participant,
      total_time_video_sec: time_diff_sec,
      total_time_video_days: round(time_diff_sec / 86400)
    }
  end)

total_video_time_df =DF.new(distinct_student_activities)

```

Let's now focus on the second piece of information that we want to gather. That is, the "entered", "replayed", "move_backward" and "finished" frequency. We will be generating a new dataset to store these pieces of information. I will be

```elixir
participants_list = DF.select(events_df, ["Participante"])
participants_list = DF.distinct(participants_list, ["Participante"])
participants_list = DF.to_rows(participants_list)
#Process each row
new_rows = 
  participants_list
  |> Enum.map(fn
    participant-> 
      participant_name = participant["Participante"]
      filtered_df = DF.filter_with(events_df, fn events_df -> Explorer.Series.equal(events_df["Participante"], participant_name) end)
      
      # filter the database based on "entered" event
      entered_df = DF.filter_with(filtered_df, fn events_df -> Explorer.Series.equal(events_df["Tipo de evento"], "entered") end)
      # filter the database based on "moved_backward" event
      move_backward_df = DF.filter_with(filtered_df, fn events_df -> Explorer.Series.equal(events_df["Tipo de evento"], "moved_backward") end)
      replayed_df = DF.filter_with(filtered_df, fn events_df -> Explorer.Series.equal(events_df["Tipo de evento"], "replayed") end)
      finished_df = DF.filter_with(filtered_df, fn events_df -> Explorer.Series.equal(events_df["Tipo de evento"], "finished") end)
      
      entered_freq = DF.n_rows(entered_df)
      move_backward_freq = DF.n_rows(move_backward_df)
      replayed_freq = DF.n_rows(replayed_df)
      finished_freq = DF.n_rows(finished_df)
      
      %{
        participant: participant_name,
        total_entered_frequency: entered_freq,
        total_playback_frequency: move_backward_freq,
        total_replayed_frequency: replayed_freq,
        total_finished_frequency: finished_freq
      }
  end)

frequency_df = DF.new(new_rows)
```

## Inspection into participation.csv

Initial inspection into **participation.csv**

```elixir
participation_df = DF.from_csv!(Constants.get_participation_file_path())
IO.inspect(participation_df)
```

This dataset contains very important information that we could use directly. Such as, time following the course, number of video reproduced, boolean variable to represent whether a course has been studied at least 75%, etc. However, there is one more information that we could calculate. That is, how long a student has been enrolled up until their latest activity. This information can be usefull for the following insight:

1. If a student has been enrolled for a long time but has very low participation (e.g. low number of video has been watched), they might be at risk of dropping out.
2. Students who have been enrolled longer might show better engagement.
3. If most students who stay enrolled for a long time don’t complete the course, it may indicate the course is not engaging or is too difficult.

```elixir
# Convert DataFrame to a list of maps
participation_map_list = DF.to_rows(participation_df)
# Process every rows 
new_rows =
  participation_map_list
  |> Enum.map(fn
    row -> 
      # compute duration in second from the first enrollment date until now 
      enrolment_duration = generate_dateTime_diff_from_str.(row["Hora ultima actividad"], row["Hora comienzo curso"])
      # recalculate the duration into n days for better interpretation
      n_days = round(enrolment_duration / 86400)
      
      row
      |>Map.put("Enrolment duration (sec)", enrolment_duration)
      |>Map.put("Enrolment duration (days)", n_days)
  end)

# Convert back to DataFrame
participation_df = DF.new(new_rows)
```

## Clustering - Profile questions

Clustering algorithms (like K-Means, DBSCAN, Hierarchical Clustering) and most ML models (like neural networks, linear regression, etc.) operate on mathematical distances, gradients, and matrix operations. Furthermore, all ML and clustering libraries in Elixir work with Nx.Tensor, hence we will be using the NX library.

We will apply cluster algorithm based on the answers provided to the profile questions of the participants. Important, this includes both the answers that are provided or not provided by the students. With the aim of gathering a better overview of all the students enrolled in the course, even when some data is incomplete.

Later on, the labels will be used as a feature parameter for the future predictive models in the supervised learning strategy. Among all the questions that we are planning to address, the profile data of the customer seems to be an important ingredient. And this approach helps address the issue of missing data: If we rely solely on the provided answers to the profile questions, we risk losing valuable information and potentially degrading model performance. By assigning students to clusters based on the available patterns, we can capture underlying similarities and trends, allowing us to enrich the dataset and build more robust models.

When building a clustering model, we have two main questions to ask ourself:

* How many cluster do we need to create to meaningfully differentiate betwwen distinct groups of students?
* Which cluster algorithm should we use? As there are many, including K-means, DBSCAN, Hierarchical clustering.

```elixir
defmodule KModes do
  @moduledoc """
  A simplified KModes clustering algorithm for categorical data.
  """

  # Public entry point
  def cluster(tensor, k, max_iter \\ 50) do
    # Convert Nx.Tensor to a list of rows
    data = Nx.to_list(tensor)
    
    # Randomly initialize k centroids
    centroids = Enum.take_random(data, k)
    
    iterate(data, centroids, k, max_iter)
  end

  # Main loop
  defp iterate(data, centroids, k, 0), do: assign_clusters(data, centroids)

  defp iterate(data, centroids, k, iter) do
    assignments = assign_clusters(data, centroids)
    new_centroids = recompute_centroids(assignments, k)

    if new_centroids == centroids do
      assignments
    else
      iterate(data, new_centroids, k, iter - 1)
    end
  end

  # Assign each item to the closest centroid using matching dissimilarity
  defp assign_clusters(data, centroids) do
    Enum.map(data, fn row ->
      {_, label} =
        centroids
        |> Enum.with_index()
        |> Enum.min_by(fn {centroid, _idx} -> matching_dissimilarity(row, centroid) end)
      
      {row, label}
    end)
  end

  # Recompute centroids as mode (most frequent value) per column
  defp recompute_centroids(assignments, k) do
    assignments
    # group by label and extract only the row from each pair to use in computing the centroids
    |> Enum.group_by(fn {_row, label} -> label end, fn {row, _label} -> row end)
    |> Enum.map(fn {_label, rows} -> column_modes(rows) end)
    |> pad_missing_centroids(k)
  end

  # Just in case some clusters were empty, it ensures k centroids are returned
  # Pad centroids if fewer than k clusters were populated
  defp pad_missing_centroids(centroids, k) do
    if length(centroids) < k do
      centroids ++ List.duplicate(List.first(centroids), k - length(centroids))
    else
      centroids
    end
  end

  # Mode for each column
  defp column_modes(rows) do
    rows
    |> Enum.zip()
    |> Enum.map(fn column ->
      column
      |> Tuple.to_list()
      |> Enum.frequencies()
      |> Enum.max_by(fn {_val, freq} -> freq end)
      |> elem(0)
    end)
  end

  # Count how many values differ (matching dissimilarity)
  defp matching_dissimilarity(row1, row2) do
    Enum.zip(row1, row2)
    |> Enum.count(fn {a, b} -> a != b end)
  end

  def silhouette_score(assignments) do
  clusters =
    assignments
    |> Enum.group_by(fn {_row, label} -> label end, fn {row, _label} -> row end)

  # Calculate silhouette score for each point
  scores =
    Enum.map(assignments, fn {point, label} ->
      a = average_dissimilarity(point, clusters[label])
      b =
        clusters
        |> Map.delete(label)
        |> Enum.map(fn {_other_label, points} -> average_dissimilarity(point, points) end)
        |> Enum.min()

      if a == b do
        0.0
      else
        (b - a) / max(a, b)
      end
    end)

  Enum.sum(scores) / length(scores)
end

# Helper function to calculate average dissimilarity to a list of points
defp average_dissimilarity(point, others) do
  others
  |> Enum.reject(&(&1 == point)) # Exclude self
  |> Enum.map(&matching_dissimilarity(point, &1))
  |> case do
    [] -> 0.0
    dissimilarities -> Enum.sum(dissimilarities) / length(dissimilarities)
  end
end

end

```

```elixir
student_profile_df
```

From this dataset we can see that the participant code column contains numerical values and string vaues. We need to convert the string value to a numerical value.

```elixir
tensor = Nx.stack(student_profile_df, axis: -1)
result = KModes.cluster(tensor,2, 500) #this returns a list of tuple {data, label}

data_list = Enum.map(result, fn {list, _label} -> list end) 
tensor_data = Nx.tensor(data_list)
IO.inspect(tensor_data)

label_list = Enum.map(result, fn {_list, label} -> label end)
tensor_label = Nx.tensor(label_list)

```

```elixir
score = KModes.silhouette_score(result)
IO.puts("Silhouette Score: #{score}")
```

```elixir
labels_list = Nx.to_flat_list(tensor_label)
cluster_df = DF.new(cluster: labels_list)
df_with_labels = DF.concat_columns(student_profile_df, cluster_df)

#DF.to_csv!(df_with_labels, "profile-clustering-2.csv")
#File.cwd!()
```

we devide cluster = 2, however, If the clusters are too broad or unhelpful, you might:

Try sub-clustering within each.

Use feature selection to refine input variables (remove irrelevant ones).

## Feature selection

* 2-> Silhouette Score: 0.4453125125238959
* 3-> Silhouette Score: 0.07989377306790536
* 4 -> Silhouette Score: 0.1313027585955213
* 5 ->  Silhouette Score: 0.14019041511784322
* 6 -> Silhouette Score: 0.11184457767867173
* 10 -> Silhouette Score: 0.048957542856680476

From this we can see that All other options (3–10) show much lower cohesion and separation, suggesting the data does not naturally support more clusters.
Even though 0.445 isn't extremely high, for categorical data using K-Modes, it’s a reasonable score, especially if the dataset is sparse or imbalanced. Let's try out feature selection.

* Use feature selection to refine input variables (remove irrelevant ones).

* ¿Cual es el país de nacimiento” -> removed, since only a couple are answered hence does not provide meaningful information
* ¿Cuál fue el nivel más alto de estudios que cursó su madre? & ¿Cuál fue el nivel más alto de estudios que cursó su padre? -> These two feature  can be combinad using max. These features are semantically related — both reflect household education background. If treated separately, they may add redundant information. Combining them reduces dimensionality and simplifies clustering.
* Chi-squared test -> for correlation. In case high correlation -> keep one that contains fewer missing values.

```elixir
#Discard "¿Cual es el país de nacimiento” column
filtered_student_col_df = DF.discard(student_profile_df, ["¿Cuál es su país de nacimiento?"])
tensor = Nx.stack(student_profile_df, axis: -1)
filtered_student_result = KModes.cluster(tensor,2, 500) #this returns a list of tuple {data, label}

data_list = Enum.map(filtered_student_result, fn {list, _label} -> list end) 
tensor_data = Nx.tensor(data_list)
IO.inspect(tensor_data)

label_list = Enum.map(filtered_student_result, fn {_list, label} -> label end)
tensor_label = Nx.tensor(label_list)
IO.inspect(tensor_label)

score = KModes.silhouette_score(filtered_student_result)
IO.puts("Silhouette Score: #{score}")
```

```elixir
# Pull each column as lists
mum_col = DF.pull(filtered_student_col_df, "¿Cuál fue el nivel más alto de estudios que cursó su madre?") |> Series.to_list() 
dad_col = DF.pull(filtered_student_col_df, "¿Cuál fue el nivel más alto de estudios que cursó su padre?") |> Series.to_list()

# Compute element-wise max
max_col = Enum.zip(mum_col, dad_col) |> Enum.map(fn {m, p} -> max(m, p) end)

# Add new column to the DataFrame
combCol_filtered_student_df = 
  filtered_student_col_df
  |> DF.put("Educación más alta de padres", max_col)
  |> DF.discard(["¿Cuál fue el nivel más alto de estudios que cursó su madre?", "¿Cuál fue el nivel más alto de estudios que cursó su padre?"])

tensor = Nx.stack(combCol_filtered_student_df, axis: -1)
combCol_filtered_student_result = KModes.cluster(tensor,2, 500) #this returns a list of tuple {data, label}

data_list = Enum.map(combCol_filtered_student_result, fn {list, _label} -> list end) 
tensor_data = Nx.tensor(data_list)
IO.inspect(tensor_data)

label_list = Enum.map(combCol_filtered_student_result, fn {_list, label} -> label end)
tensor_label = Nx.tensor(label_list)
IO.inspect(tensor_label)

score = KModes.silhouette_score(combCol_filtered_student_result)
IO.puts("Silhouette Score: #{score}")
```

```elixir
labels_list = Nx.to_flat_list(tensor_label)
cluster_df = DF.new(cluster: labels_list)
df_with_labels = DF.concat_columns(combCol_filtered_student_df, cluster_df)

#DF.to_csv!(df_with_labels, "profile-clustering-filterCol-combCol-2.csv")
#File.cwd!()
```

Removing the group of people who are inactive (in cluster 0). And perform sub-clustering

```elixir
#Remove cluster 1 data
cluster_label_df = DF.from_csv!(Constants.get_clustering_label_file_path())

cluster_0_df = DF.filter_with(cluster_label_df, fn events_df -> Explorer.Series.equal(events_df["cluster"], 0) end)
cluster_1_df = DF.filter_with(cluster_label_df, fn events_df -> Explorer.Series.equal(events_df["cluster"], 1) end) |> DF.discard(["cluster"])

tensor = Nx.stack(cluster_1_df, axis: -1)
combCol_filtered_student_result = KModes.cluster(tensor,2, 500) #this returns a list of tuple {data, label}

data_list = Enum.map(combCol_filtered_student_result, fn {list, _label} -> list end) 
tensor_data = Nx.tensor(data_list)
IO.inspect(tensor_data)

label_list = Enum.map(combCol_filtered_student_result, fn {_list, label} -> label end)
tensor_label = Nx.tensor(label_list)
IO.inspect(tensor_label)

score = KModes.silhouette_score(combCol_filtered_student_result)
IO.puts("Silhouette Score: #{score}")
```

```elixir
labels_list = Nx.to_flat_list(tensor_label)
cluster_df = DF.new(cluster: labels_list)
df_with_labels = DF.concat_columns(cluster_1_df, cluster_df)

#DF.to_csv!(df_with_labels, "profile-sub-clustering-2.csv")
#File.cwd!()
```

```elixir
cluster_label_df = DF.from_csv!(Constants.get_clustering_label_file_path())
cluster_label_df = DF.filter_with(cluster_label_df, fn events_df -> Explorer.Series.equal(events_df["cluster"], 0) end)

grouped_age_df =
  cluster_label_df
    |> DF.group_by("Educación más alta de padres")
    |> DF.summarise_with(&[count: Series.count(&1["Educación más alta de padres"])])

```

## Decision tree

Now that we have the clusters, we can use these label as one of the features for our binary tree model. Looking for answers for the following questions: Which questions are more likely to be not responded by the student.

```elixir
cluster_label_df = DF.from_csv!(Constants.get_clustering_label_file_path())

entered_df = DF.filter_with(cluster_label_df, fn events_df -> Explorer.Series.equal(events_df["cluster"], 0) end)
```

<!-- livebook:{"offset":31503,"stamp":{"token":"XCP.-uLj2Ad5TDdvh8frNhurQFFfAJ4FrswRRMYeeeEX-qmOxAfdlALVWHOx7I-Qm3mpJ1VM1DddV9E_N5Q-pKMGkzXpddQS9IxRBW5rFA","version":2}} -->
